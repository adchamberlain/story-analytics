# Story Data Context Specification

**Version:** 0.1.0-draft
**Date:** 2026-02-09
**Status:** Design draft — open for iteration

---

## Design Philosophy

This format is designed for an AI-native world where:

1. **LLMs are the primary consumer**, not BI tool field pickers. The format must be rich enough for an LLM to reason about business logic, not just structured enough for a compiler.
2. **Agents do the grunt work.** The format is designed to be auto-generated by crawling schemas, parsing LookML/dbt, profiling data, and enriching with AI. Humans review and bless, not author from scratch.
3. **Metrics are both compilable AND contextual.** We support deterministic SQL compilation for guaranteed consistency on key metrics, AND rich unstructured context for flexible LLM reasoning on ad-hoc questions.
4. **Validation is a first-class concept.** Every metric can carry known-good query-result pairs — a test suite, not just a definition. The system validates outputs against benchmarks.
5. **The format is portable.** Not locked to any BI tool. Bidirectional converters to/from LookML, dbt MetricFlow, Cube.js are a design goal.

### Key Insight: Compile-Time + Runtime Hybrid

Traditional semantic layers are compile-time: rigid YAML → deterministic SQL. An AI-native data context is runtime: rich context → LLM generates SQL → validated against known-good results.

We support **both modes**:
- **Compiled queries:** For core metrics (revenue, active users, churn), the metric compiler generates guaranteed-correct SQL from definitions. No LLM in the path. Instant, deterministic, auditable.
- **Contextual queries:** For ad-hoc questions ("why did revenue drop?"), the LLM uses the data context as a knowledge base — reading descriptions, business rules, data quirks, validated examples — to generate appropriate SQL. The output is then validated against metric definitions.

This hybrid approach gives you the consistency of MetricFlow with the flexibility of an LLM analyst.

---

## Format Overview

A Story data context is a directory containing YAML files:

```
data_context/
├── metadata.yaml           # Metadata, source info, adapter config
├── tables/                 # One file per table (or grouped)
│   ├── orders.yaml
│   ├── customers.yaml
│   └── subscriptions.yaml
├── metrics.yaml            # Metric definitions
├── joins.yaml              # Join graphs / join definitions
├── knowledge/              # Rich context for LLM consumption
│   ├── business_context.yaml
│   ├── data_quirks.yaml
│   └── validated_examples.yaml
└── validation/             # Test suite
    └── benchmarks.yaml
```

The format uses YAML for human readability and LLM parseability. Every file is independently valid — you can load just the tables, or just the metrics, or just the knowledge base.

---

## 1. Metadata

The metadata file describes the data context: where it came from, what database it targets, and configuration.

```yaml
# metadata.yaml
story_data_context: "0.1.0"

metadata:
  name: "acme-analytics"
  description: "Data context for Acme Corp's analytics warehouse"
  created_at: "2026-02-15T10:30:00Z"
  updated_at: "2026-02-15T10:30:00Z"

  # Provenance — how was this generated?
  source:
    type: lookml                          # lookml | dbt | schema | manual | hybrid
    origin: "github.com/acme/lookml-repo" # Where the source material came from
    extraction_date: "2026-02-15"
    extraction_method: "story extract v0.1"
    confidence: 0.92                      # Overall extraction confidence (0-1)

# Database adapter configuration
adapter:
  type: snowflake                         # snowflake | bigquery | redshift | postgres | duckdb
  default_schema: analytics
  default_database: ACME_DW

  # SQL dialect rules (replaces our dialect.yaml)
  dialect:
    date_trunc: "DATE_TRUNC('{grain}', {expr})"
    current_timestamp: "CURRENT_TIMESTAMP()"
    null_safe_divide: "{num} / NULLIF({den}, 0)"
    string_agg: "LISTAGG({expr}, ', ')"
    supports_qualify: true
    supports_pivot: false

# Time spine configuration (required for cumulative metrics)
time_spine:
  table: analytics.time_spine
  column: date_day
  granularity: day
  range: ["2020-01-01", "2030-01-01"]
```

### Metadata Properties

| Property | Required | Description |
|---|---|---|
| `story_data_context` | Yes | Spec version for compatibility checking |
| `metadata.name` | Yes | Unique name for this data context |
| `metadata.source` | No | Provenance — how this was generated |
| `metadata.source.type` | No | `lookml`, `dbt`, `schema`, `manual`, `hybrid` |
| `metadata.source.confidence` | No | Extraction confidence score (0-1) |
| `adapter.type` | Yes | Target database type |
| `adapter.dialect` | No | SQL dialect overrides |
| `time_spine` | No | Required for cumulative metrics |

---

## 2. Tables

A table represents a table or view in the warehouse. It contains typed columns organized as **entities** (join keys), **dimensions** (group-by columns), and **measures** (aggregations).

Tables are the atoms of the data context. Everything else — metrics, joins, knowledge — references tables.

```yaml
# tables/orders.yaml
tables:

  - name: order_items
    table: analytics.fct_order_items    # Fully qualified table reference
    description: >
      Line-item level fact table. Each row is one product in one order.
      Primary source for revenue, GMV, and product performance metrics.
      Contains ~110K rows spanning 2016-10 to 2018-08.

    # What LookML calls the "primary key"
    grain: order_item_id                # The column(s) that uniquely identify a row

    # Default time dimension for measures (like dbt's agg_time_dimension)
    default_time_dimension: order_date

    # Tags for categorization and discovery
    tags: [revenue, fact, core]
    domain: revenue                     # Business domain: revenue, growth, product, etc.

    # --- ENTITIES (join keys) ---
    entities:
      - name: order_item
        type: primary
        expr: order_id || '-' || CAST(order_item_id AS VARCHAR)

      - name: order
        type: foreign
        expr: order_id
        references: orders.order        # Explicit reference to target table.entity

      - name: customer
        type: foreign
        expr: customer_id
        references: customers.customer

      - name: product
        type: foreign
        expr: product_id
        references: products.product

      - name: seller
        type: foreign
        expr: seller_id
        references: sellers.seller

    # --- DIMENSIONS ---
    dimensions:

      # Time dimension with granularity support
      - name: order_date
        type: time
        expr: order_date                # Column name (omit if same as name)
        granularities: [day, week, month, quarter, year]
        description: "Date the order was placed"

      # Categorical dimension
      - name: order_status
        type: categorical
        description: >
          Order lifecycle status. Values: delivered (97%), shipped (1.1%),
          canceled (0.6%), unavailable (0.6%), invoiced, processing.
        # AI-enrichment: known values and their distribution
        known_values:
          - value: delivered
            frequency: 0.97
          - value: shipped
            frequency: 0.011
          - value: canceled
            frequency: 0.006

      - name: product_category
        type: categorical
        description: >
          English product category name. 73 categories. Top: bed_bath_table,
          sports_leisure, furniture_decor, health_beauty.
        cardinality: 73
        tags: [product, high-cardinality]

      - name: customer_state
        type: categorical
        description: >
          Two-letter Brazilian state code. SP (42%), RJ (12.9%), MG (11.7%).
        cardinality: 27
        tags: [geographic]

      # Boolean dimension (LookML's yesno)
      - name: is_delivered
        type: boolean
        expr: "order_status = 'delivered'"
        description: "Whether the order was successfully delivered"

      # Bucketed/tier dimension (LookML's tier)
      - name: price_tier
        type: tier
        expr: price
        tiers: [0, 25, 50, 100, 250, 500]
        labels: ["Under $25", "$25-50", "$50-100", "$100-250", "$250-500", "$500+"]

    # --- MEASURES ---
    measures:

      - name: gmv
        agg: sum
        expr: price
        label: "Gross Merchandise Value"
        description: >
          Sum of product prices excluding freight. Filter to delivered
          orders for realized GMV. This is the primary revenue metric.
        format: currency
        format_pattern: "R$ #,##0.00"
        tags: [revenue, core]

      - name: freight_revenue
        agg: sum
        expr: freight_value
        label: "Freight Revenue"
        format: currency

      - name: total_revenue
        agg: sum
        expr: "price + freight_value"
        label: "Total Revenue"
        description: "Sum of product price plus freight for all line items"
        format: currency

      - name: item_count
        agg: count
        label: "Items Sold"

      - name: order_count
        agg: count_distinct
        expr: order_id
        label: "Order Count"

      - name: unique_customers
        agg: count_distinct
        expr: customer_id
        label: "Unique Customers"

      - name: avg_item_price
        agg: average
        expr: price
        label: "Avg Item Price"
        format: currency

      - name: median_item_price
        agg: median
        expr: price
        label: "Median Item Price"
        description: >
          Median selling price (R$74.99). More representative than mean
          (R$120.65) due to right-skewed distribution.
        format: currency

      # Filtered measure (LookML's filters parameter)
      - name: delivered_gmv
        agg: sum
        expr: price
        filter: "order_status = 'delivered'"
        label: "Realized GMV"
        description: "GMV for delivered orders only"
        format: currency

      # Semi-additive measure (dbt's non_additive_dimension)
      # Uncommon in this dataset, but showing the pattern:
      # - name: mrr
      #   agg: sum
      #   expr: subscription_value
      #   semi_additive:
      #     time_dimension: snapshot_date
      #     window: latest              # latest | earliest
      #     partition_by: [customer_id] # Optional

      # Percentile measure
      - name: p95_item_price
        agg: percentile
        expr: price
        percentile: 0.95
        label: "95th Percentile Price"
        format: currency
```

### Table Properties

| Property | Required | Description |
|---|---|---|
| `name` | Yes | Unique table name |
| `table` | Yes | Fully qualified table/view reference |
| `description` | Recommended | Rich description (consumed by LLMs) |
| `grain` | Recommended | Column(s) that uniquely identify a row |
| `default_time_dimension` | No | Default time dimension for measures |
| `tags` | No | Tags for categorization and search |
| `domain` | No | Business domain (revenue, growth, product, etc.) |

### Entity Properties

| Property | Required | Description |
|---|---|---|
| `name` | Yes | Entity name (used as join key identifier) |
| `type` | Yes | `primary`, `unique`, `foreign`, `natural` |
| `expr` | No | Column expression (defaults to `name`) |
| `references` | No | Target table.entity for foreign keys |

### Dimension Properties

| Property | Required | Description |
|---|---|---|
| `name` | Yes | Dimension name |
| `type` | Yes | `categorical`, `time`, `boolean`, `tier`, `numeric`, `location` |
| `expr` | No | SQL expression (defaults to column = name) |
| `description` | Recommended | Rich description |
| `granularities` | For `time` | Available time granularities |
| `known_values` | No | Enumerated values with optional frequency |
| `cardinality` | No | Approximate distinct value count |
| `tiers` | For `tier` | Bucket boundaries |
| `labels` | For `tier` | Human-readable bucket labels |
| `tags` | No | Tags for categorization |
| `hidden` | No | If true, not shown in catalog UI |

### Dimension Types

| Type | Description | LookML Equivalent | dbt Equivalent |
|---|---|---|---|
| `categorical` | String, enum, or any groupable column | `type: string` | `type: categorical` |
| `time` | Timestamp/date with granularity support | `dimension_group type: time` | `type: time` |
| `boolean` | True/false derived from expression | `type: yesno` | `type: categorical` + expr |
| `tier` | Numeric bucketing | `type: tier` | N/A (manual) |
| `numeric` | Numeric non-aggregated column | `type: number` | `type: categorical` |
| `location` | Geographic coordinates | `type: location` | N/A |

### Measure Properties

| Property | Required | Description |
|---|---|---|
| `name` | Yes | Unique measure name (across all tables) |
| `agg` | Yes | Aggregation type (see table below) |
| `expr` | No | Column or SQL expression (defaults to name) |
| `label` | Recommended | Human-readable label |
| `description` | Recommended | Rich description |
| `format` | No | Format type: `currency`, `percent`, `number`, `integer` |
| `format_pattern` | No | Specific format pattern (e.g., `"$#,##0.00"`) |
| `filter` | No | SQL WHERE condition applied to this measure |
| `tags` | No | Tags for categorization |
| `semi_additive` | No | Semi-additive configuration (see below) |
| `percentile` | For `percentile` agg | Float 0-1 |

### Aggregation Types

| `agg` | SQL | Notes |
|---|---|---|
| `sum` | `SUM(expr)` | |
| `count` | `COUNT(*)` or `COUNT(expr)` | Use `count_distinct` for unique counts |
| `count_distinct` | `COUNT(DISTINCT expr)` | |
| `average` | `AVG(expr)` | |
| `min` | `MIN(expr)` | |
| `max` | `MAX(expr)` | |
| `median` | Dialect-specific | 50th percentile |
| `percentile` | Dialect-specific | Requires `percentile` property |
| `sum_boolean` | `SUM(CASE WHEN expr THEN 1 ELSE 0 END)` | For boolean flags |
| `list` | `LISTAGG(expr)` / `STRING_AGG(expr)` | Dialect-specific |

### Semi-Additive Measures

For measures like MRR or account balances that shouldn't be summed across time:

```yaml
measures:
  - name: mrr
    agg: sum
    expr: subscription_value
    semi_additive:
      time_dimension: snapshot_date
      window: latest            # latest | earliest
      partition_by: [user_id]   # Optional: partition before windowing
```

Equivalent to dbt's `non_additive_dimension` and LookML patterns using derived tables with window functions.

---

## 3. Metrics

Metrics are named, governed business definitions built on measures. They are the primary interface for querying — users and LLMs request metrics, not raw measures.

### Design Principle: Dual Nature

Each metric is both:
1. **A compilation target** — the metric compiler can generate deterministic SQL
2. **A knowledge artifact** — with rich context an LLM can use for reasoning

```yaml
# metrics.yaml
metrics:

  # =========================================================================
  # SIMPLE METRICS — wrap a single measure, optionally with a filter
  # =========================================================================

  - name: gmv
    type: simple
    label: "Gross Merchandise Value"
    description: >
      Total gross merchandise value — sum of product prices excluding freight.
      Includes all order statuses. For realized revenue, use realized_gmv
      which filters to delivered orders only.
    measure: gmv                          # References a measure by name
    tags: [revenue, core, executive]
    domain: revenue

    # Owner — who is responsible for this metric's definition
    owner: analytics-team

    # Tier — how governed/critical is this metric
    tier: certified                       # certified | reviewed | draft | deprecated

    # Format override (inherits from measure if not set)
    format: currency

  - name: realized_gmv
    type: simple
    label: "Realized GMV"
    description: >
      GMV for delivered orders only. Represents actual revenue from
      successfully completed transactions. ~97% of total GMV.
    measure: gmv
    filter: "order_status = 'delivered'"
    tags: [revenue, core]
    tier: certified

  # =========================================================================
  # DERIVED METRICS — expressions over other metrics
  # =========================================================================

  - name: average_order_value
    type: derived
    label: "Average Order Value (AOV)"
    description: >
      Average revenue per order. Calculated as GMV / distinct order count.
      Dataset average is approximately R$137. This is a key efficiency metric
      tracked by the executive team weekly.
    expression: gmv / NULLIF(orders, 0)
    inputs:
      - metric: gmv
      - metric: order_count
        alias: orders
    tags: [revenue, efficiency, executive]
    tier: certified

  # =========================================================================
  # RATIO METRICS — numerator / denominator
  # =========================================================================

  - name: five_star_rate
    type: ratio
    label: "5-Star Review Rate"
    description: >
      Percentage of reviews that are 5-star. Benchmark: ~57%.
    numerator: five_star_count
    denominator: review_count
    format: percent
    tags: [satisfaction]

  # =========================================================================
  # CUMULATIVE METRICS — running totals and grain-to-date
  # =========================================================================

  - name: cumulative_gmv
    type: cumulative
    label: "Cumulative GMV"
    description: "All-time running total of GMV"
    measure: gmv
    # No window = all-time accumulation

  - name: gmv_mtd
    type: cumulative
    label: "GMV Month-to-Date"
    description: "GMV accumulated from the start of each month"
    measure: gmv
    grain_to_date: month

  - name: trailing_30d_orders
    type: cumulative
    label: "Orders (Trailing 30 Days)"
    description: "Rolling 30-day order count"
    measure: order_count
    window: 30 days

  # =========================================================================
  # GROWTH / PERIOD-OVER-PERIOD METRICS
  # =========================================================================

  - name: gmv_growth_mom
    type: derived
    label: "GMV Growth % (MoM)"
    description: >
      Month-over-month percentage change in GMV. Positive = growth.
    expression: (current_gmv - prior_gmv) * 100.0 / NULLIF(prior_gmv, 0)
    inputs:
      - metric: gmv
        alias: current_gmv
      - metric: gmv
        alias: prior_gmv
        offset: 1 month
    format: percent
    tags: [growth, executive]

  - name: gmv_yoy
    type: derived
    label: "GMV Year-over-Year"
    expression: (current - prior) * 100.0 / NULLIF(prior, 0)
    inputs:
      - metric: gmv
        alias: current
      - metric: gmv
        alias: prior
        offset: 1 year
    format: percent

  # =========================================================================
  # CONVERSION METRICS
  # =========================================================================

  - name: visit_to_purchase_rate
    type: conversion
    label: "Visit-to-Purchase Conversion"
    description: "Rate of visits that result in a purchase within 7 days"
    base_measure: visit_count
    conversion_measure: order_count
    entity: customer
    window: 7 days
    format: percent
```

### Metric Properties (All Types)

| Property | Required | Description |
|---|---|---|
| `name` | Yes | Unique metric name |
| `type` | Yes | `simple`, `derived`, `ratio`, `cumulative`, `conversion` |
| `label` | Recommended | Human-readable label |
| `description` | Recommended | Rich description (for LLM and catalog) |
| `tags` | No | Tags for categorization and search |
| `domain` | No | Business domain |
| `owner` | No | Team or person responsible |
| `tier` | No | Governance level: `certified`, `reviewed`, `draft`, `deprecated` |
| `format` | No | `currency`, `percent`, `number`, `integer` |
| `format_pattern` | No | Specific format (e.g., `"$#,##0.00"`) |
| `filter` | No | SQL WHERE condition |

### Type-Specific Properties

**Simple:**
| Property | Required | Description |
|---|---|---|
| `measure` | Yes | Name of the measure to expose |
| `filter` | No | Additional SQL WHERE condition |

**Derived:**
| Property | Required | Description |
|---|---|---|
| `expression` | Yes | SQL expression using input aliases |
| `inputs` | Yes | List of input metrics |
| `inputs[].metric` | Yes | Metric name |
| `inputs[].alias` | No | Alias for use in expression |
| `inputs[].offset` | No | Time offset: `1 month`, `1 year`, `7 days` |
| `inputs[].filter` | No | Filter on this specific input |

**Ratio:**
| Property | Required | Description |
|---|---|---|
| `numerator` | Yes | Metric name or `{name, filter}` object |
| `denominator` | Yes | Metric name or `{name, filter}` object |

**Cumulative:**
| Property | Required | Description |
|---|---|---|
| `measure` | Yes | Measure to accumulate |
| `window` | No | Rolling window: `7 days`, `30 days`, `3 months` |
| `grain_to_date` | No | Accumulate from grain start: `month`, `quarter`, `year` |

**Conversion:**
| Property | Required | Description |
|---|---|---|
| `base_measure` | Yes | The starting event measure |
| `conversion_measure` | Yes | The conversion event measure |
| `entity` | Yes | Entity linking base and conversion |
| `window` | No | Max time between events |

---

## 4. Joins (Join Graphs)

Joins define how tables are joined together. They are the entry points for queries.

### Design: Explicit Joins with Inferred Defaults

We combine the best of LookML (explicit join definitions with relationship types) and dbt (entity-based implicit joins). Entities provide the default join graph; joins can override or extend it.

```yaml
# joins.yaml
joins:

  - name: order_analysis
    description: >
      Primary join graph for revenue and order analysis. Joins order items
      with customer demographics, product categories, and seller info.
    base_table: order_items

    # Tags and access control
    tags: [revenue, core]
    access: public                        # public | restricted | internal

    joins:
      - table: customers
        on: "${order_items.customer_id} = ${customers.customer_id}"
        relationship: many_to_one
        type: left                        # left | inner | full | cross
        description: "Customer who placed the order"

      - table: products
        on: "${order_items.product_id} = ${products.product_id}"
        relationship: many_to_one
        type: left

      - table: sellers
        on: "${order_items.seller_id} = ${sellers.seller_id}"
        relationship: many_to_one
        type: left

      # Self-join example (join same table twice under different alias)
      - table: customers
        alias: referring_customer
        on: "${order_items.referrer_id} = ${referring_customer.customer_id}"
        relationship: many_to_one
        type: left

    # Mandatory filters — always applied
    always_filter:
      order_date: "last 2 years"

    # Default filters — applied unless user overrides
    default_filters:
      order_status: "delivered"

    # Row-level security
    access_filters:
      - dimension: customer_state
        user_attribute: allowed_states

    # Hidden SQL filter — always applied, not visible to users
    sql_always_where: "order_date >= '2016-01-01'"

  - name: customer_360
    description: "Complete customer view with orders, reviews, and payments"
    base_table: customers
    joins:
      - table: orders
        on: "${customers.customer_id} = ${orders.customer_id}"
        relationship: one_to_many
        type: left
      - table: reviews
        on: "${orders.order_id} = ${reviews.order_id}"
        relationship: one_to_many
        type: left
```

### Join Properties (Top-Level)

| Property | Required | Description |
|---|---|---|
| `name` | Yes | Unique join graph name |
| `base_table` | Yes | The primary table (FROM table) |
| `description` | Recommended | Rich description |
| `joins` | No | Explicit join definitions |
| `always_filter` | No | Mandatory filters (user can change value, not remove) |
| `default_filters` | No | Default filters (user can override) |
| `access_filters` | No | Row-level security filters |
| `sql_always_where` | No | Hidden SQL WHERE clause |
| `access` | No | Visibility level |
| `tags` | No | Tags |

### Join Entry Properties

| Property | Required | Description |
|---|---|---|
| `table` | Yes | Table to join |
| `on` | Yes | SQL ON condition using `${table.column}` syntax |
| `relationship` | Yes | `many_to_one`, `one_to_one`, `one_to_many`, `many_to_many` |
| `type` | No | `left` (default), `inner`, `full`, `cross` |
| `alias` | No | Alias for joining the same table multiple times |
| `description` | No | Join description |
| `fields` | No | Restrict which fields are available from this join |

### Relationship Types

Getting relationships right is critical for aggregation correctness. A wrong relationship type leads to double-counting.

| Relationship | When to use | Aggregation behavior |
|---|---|---|
| `many_to_one` | Fact → Dimension (most common) | Safe to aggregate — no fan-out |
| `one_to_one` | Dimension → Dimension | Safe — unique match |
| `one_to_many` | Dimension → Fact | Causes fan-out — aggregations may need DISTINCT |
| `many_to_many` | Fact → Fact (rare) | Dangerous — requires careful handling |

---

## 5. Knowledge Base

This is what makes the format AI-native. The knowledge base contains rich, unstructured context that an LLM uses when reasoning about the data. It's not consumed by the compiler — it's consumed by the AI.

### Business Context

```yaml
# knowledge/business_context.yaml
business_context:

  overview: >
    Acme Corp is a Brazilian e-commerce marketplace connecting SMB sellers
    with consumers. Revenue model: Acme takes a commission on each sale plus
    freight fees. The marketplace operates exclusively in Brazil.

  # Named business rules that the LLM should know about
  rules:
    - name: revenue_recognition
      description: >
        Revenue is recognized at order delivery, not at purchase. Use
        'delivered' status filter for realized revenue metrics. Approximately
        3% of orders are canceled or unavailable.

    - name: customer_identity
      description: >
        CRITICAL: Always use customer_unique_id for customer counts, NOT
        customer_id. A single person can have multiple customer_ids (one per
        order in some cases). customer_unique_id is deduplicated.

    - name: currency
      description: >
        All monetary values are in Brazilian Reais (BRL / R$). Do not
        convert to USD unless explicitly requested.

    - name: historical_data
      description: >
        Data spans 2016-10 to 2018-08 only. Do not use CURRENT_DATE or
        relative date filters like "last 12 months" — use explicit date
        ranges within the data window.

  # Domain-specific glossary
  glossary:
    GMV: "Gross Merchandise Value — total product prices excluding freight"
    AOV: "Average Order Value — GMV divided by order count"
    Boleto: "Brazilian bank payment slip — unique to Brazil, ~20% of payments"
    Installments: "Credit card installment payments — common in Brazil, avg 3-4 installments"
    Realized GMV: "GMV for delivered orders only (excludes canceled/unavailable)"

  # Key performance indicators the business tracks
  kpis:
    - name: GMV
      target: "Growth month-over-month"
      current_benchmark: "~R$850K/month at peak"
    - name: AOV
      target: "Maintain above R$130"
      current_benchmark: "R$137 average"
    - name: On-time delivery rate
      target: "Above 90%"
      current_benchmark: "~63% (major improvement area)"

  # Naming conventions (critical for consistent metric generation)
  naming_conventions:
    - "Use 'GMV' not 'sales' or 'revenue' for gross merchandise value"
    - "Use 'AOV' not 'average_order_value' as the abbreviation"
    - "Use 'Boleto' not 'bank_slip' or 'bank_transfer' for boleto bancário"
    - "Prefix growth metrics with the base metric: gmv_growth_mom, not monthly_growth"
    - "Use _rate suffix for ratios expressed as percentages"
    - "Use _count suffix for count metrics"
```

### Data Quirks

```yaml
# knowledge/data_quirks.yaml
data_quirks:

  # Column-level quirks that affect query correctness
  columns:
    - table: order_items
      column: price
      quirk: >
        Price is per-item, not per-order. An order with 3 items has 3 rows.
        SUM(price) gives GMV, not per-order revenue. For per-order revenue,
        use the orders table which pre-aggregates.

    - table: customers
      column: customer_id
      quirk: >
        NOT a unique customer identifier. A single person can appear with
        multiple customer_ids. Always use customer_unique_id for counting
        unique customers. This is a known data quality issue.

    - table: reviews
      column: review_score
      quirk: >
        Only ~10% of orders have reviews. The review sample is biased toward
        extreme experiences (more 5-star and 1-star than middle scores).
        Do not treat review metrics as representative of all orders.

  # Date range limitations
  date_ranges:
    - table: order_items
      column: order_date
      min: "2016-10-01"
      max: "2018-08-31"
      note: "Only ~22 months of data. No data outside this range."

  # Known data quality issues
  quality_issues:
    - severity: warning
      description: >
        product_category is NULL for ~0.5% of items. These are from
        sellers who didn't properly categorize their products.
    - severity: info
      description: >
        customer_city values are inconsistent (different spellings of
        same city). Use customer_state for reliable geographic analysis.
```

### Validated Examples

This is the "test suite" concept from the conversation — known-good query-result pairs that the LLM can learn from and that the system can validate against.

```yaml
# knowledge/validated_examples.yaml
validated_examples:

  # Each example is a known-good query with its expected result
  - name: "total_gmv_all_time"
    description: "Total GMV across all orders and statuses"
    metric: gmv
    filters: {}
    expected_result: 13591643.70
    tolerance: 0.01                       # Allow 1% variance
    validated_at: "2026-02-15"
    sql: >
      SELECT SUM(price) FROM analytics.fct_order_items
    notes: "This is the canonical GMV number. If your query doesn't match, something is wrong."

  - name: "gmv_delivered_only"
    description: "Realized GMV (delivered orders)"
    metric: realized_gmv
    filters:
      order_status: "delivered"
    expected_result: 12741025.33
    tolerance: 0.01
    validated_at: "2026-02-15"

  - name: "orders_by_month_sample"
    description: "Order count for a specific month"
    metric: order_count
    filters:
      order_date: "2017-11"
    group_by: [order_date__month]
    expected_result:
      - { order_date__month: "2017-11", order_count: 7544 }
    validated_at: "2026-02-15"

  - name: "aov_sanity_check"
    description: "AOV should be approximately R$137"
    metric: average_order_value
    expected_result: 137.0
    tolerance: 0.05                       # 5% tolerance (approximate)
    validated_at: "2026-02-15"
    notes: "If AOV is wildly different, check whether you're dividing by item count vs order count"

  # Annotated SQL examples — not just result validation, but teaching material
  - name: "revenue_by_category_pattern"
    description: "How to correctly calculate revenue by product category"
    purpose: example                      # example | benchmark | regression
    sql: >
      SELECT
        product_category,
        SUM(price) AS gmv,
        COUNT(DISTINCT order_id) AS orders,
        SUM(price) / NULLIF(COUNT(DISTINCT order_id), 0) AS aov
      FROM analytics.fct_order_items
      WHERE order_status = 'delivered'
      GROUP BY 1
      ORDER BY gmv DESC
      LIMIT 10
    annotations:
      - line: 5
        note: "Use COUNT(DISTINCT order_id) not COUNT(*) — multiple items per order"
      - line: 6
        note: "NULLIF prevents division by zero"
      - line: 8
        note: "Filter to delivered for realized revenue"
```

---

## 6. Validation (Benchmarks)

Separate from knowledge/validated_examples, the validation directory contains machine-readable test definitions that can be run automatically.

```yaml
# validation/benchmarks.yaml
benchmarks:

  # Metric value benchmarks — run these to verify the data context is correct
  - metric: gmv
    tests:
      - name: "all_time_total"
        expected: 13591643.70
        tolerance_pct: 1
      - name: "delivered_only"
        filter: "order_status = 'delivered'"
        expected: 12741025.33
        tolerance_pct: 1
      - name: "not_negative"
        assertion: "> 0"

  - metric: order_count
    tests:
      - name: "all_time_total"
        expected: 98666
        tolerance_pct: 1
      - name: "reasonable_range"
        assertion: "BETWEEN 90000 AND 110000"

  - metric: average_order_value
    tests:
      - name: "sanity_check"
        expected: 137.0
        tolerance_pct: 5
      - name: "not_extreme"
        assertion: "BETWEEN 50 AND 500"

  # Relationship integrity tests
  - type: relationship_test
    name: "order_items_to_orders"
    test: "Every order_id in order_items exists in orders"
    sql: >
      SELECT COUNT(*) FROM analytics.fct_order_items oi
      LEFT JOIN analytics.fct_orders o ON oi.order_id = o.order_id
      WHERE o.order_id IS NULL
    expected: 0

  # Freshness tests
  - type: freshness_test
    name: "orders_data_freshness"
    sql: "SELECT MAX(order_date) FROM analytics.fct_order_items"
    warn_after: "2018-08-01"
```

---

## 7. Access Control

Access control definitions can live at any level — table, dimension, measure, metric, or join.

```yaml
# Can be in metadata.yaml or a separate access.yaml
access_control:

  # Define grants based on user attributes
  grants:
    - name: financial_data_access
      user_attribute: department
      allowed_values: [finance, executive, data_engineering]

    - name: pii_access
      user_attribute: pii_level
      allowed_values: [full]

  # Apply grants at different levels
  field_restrictions:
    - table: customers
      dimensions: [email, phone, address]
      required_grants: [pii_access]

    - table: financials
      required_grants: [financial_data_access]

  # Row-level security
  row_filters:
    - join: order_analysis
      dimension: customer_state
      user_attribute: allowed_states
```

---

## Cross-Format Mapping

How our concepts map to other formats:

| Story | LookML | dbt MetricFlow | Cube.js |
|---|---|---|---|
| `table` | `view` | `semantic_model` | `cube` |
| `dimension` (categorical) | `dimension type: string` | `dimension type: categorical` | `dimension type: string` |
| `dimension` (time) | `dimension_group type: time` | `dimension type: time` | `dimension type: time` |
| `dimension` (boolean) | `dimension type: yesno` | (categorical + expr) | `dimension type: boolean` |
| `dimension` (tier) | `dimension type: tier` | (N/A — manual) | (N/A — case statement) |
| `measure` | `measure` | `measure` | `measure` |
| `measure.filter` | `measure filters:` | (N/A at measure level) | `measure filters:` |
| `measure.semi_additive` | (derived table pattern) | `non_additive_dimension` | (N/A) |
| `metric` (simple) | (N/A — measures are metrics) | `metric type: simple` | (N/A — measures are metrics) |
| `metric` (derived) | `measure type: number` | `metric type: derived` | `measure type: number` |
| `metric` (ratio) | `measure type: number` | `metric type: ratio` | `measure type: number` |
| `metric` (cumulative) | `measure type: running_total` | `metric type: cumulative` | `measure rollingWindow` |
| `metric` (conversion) | (N/A) | `metric type: conversion` | (N/A) |
| `join` (top-level) | `explore` | (implicit via entities) | (implicit via joins) |
| `join.joins[]` | `join` | (entity-based) | `joins` |
| `knowledge` | (descriptions only) | (descriptions only) | (N/A) |
| `validated_examples` | (N/A) | (N/A) | (N/A) |
| `access_control` | `access_grant` + `required_access_grants` | (N/A) | `SECURITY_CONTEXT` |
| `tier` (governance) | (N/A) | (N/A) | (N/A) |

**Unique to Story format:**
- `knowledge/` — Rich business context, data quirks, naming conventions
- `validated_examples` — Known-good query-result pairs as a test suite
- `metric.tier` — Governance levels (certified, reviewed, draft, deprecated)
- `metric.owner` — Metric ownership
- `dimension.known_values` — Enumerated values with frequency distribution
- `metadata.source` — Extraction provenance and confidence scores

---

## LookML Extraction Mapping

When extracting from LookML, here's how concepts map:

```
LookML view                    → Story table
  sql_table_name               → table.source (fully qualified table ref)
  derived_table.sql            → table.source (materialize or inline)
  dimension                    → table.dimensions[]
  dimension_group type: time   → table.dimensions[] type: time
  dimension type: yesno        → table.dimensions[] type: boolean
  dimension type: tier         → table.dimensions[] type: tier
  measure type: sum            → table.measures[] agg: sum
  measure type: count          → table.measures[] agg: count
  measure type: count_distinct → table.measures[] agg: count_distinct
  measure type: number         → metrics[] type: derived (promoted to metric)
  measure filters:             → table.measures[].filter
  primary_key: yes             → table.grain
  value_format_name            → format + format_pattern

LookML explore                 → Story join
  join                         → join.joins[]
  join.type                    → join entry.type (left_outer → left)
  join.relationship            → join entry.relationship
  join.sql_on                  → join entry.on
  always_filter                → join.always_filter
  conditionally_filter         → join.default_filters (simplified)
  access_filter                → join.access_filters
  sql_always_where             → join.sql_always_where
  required_access_grants       → access_control

LookML model                   → Story metadata (partial)
  connection                   → adapter.type
  datagroup                    → (not directly mapped — persistence is adapter-level)
  named_value_format           → (absorbed into format_pattern)
  access_grant                 → access_control.grants

LookML (no equivalent)         → Story knowledge/
                               → Story validated_examples
                               → Story metric.tier / metric.owner
```

### What Gets Lost in Extraction

These LookML features have no direct equivalent and require special handling:

| LookML Feature | Handling Strategy |
|---|---|
| **Liquid templating** | Evaluate at extraction time if possible; flag as `needs_review` if dynamic |
| **Parameters** | Convert to dimension with known_values if static; flag if truly dynamic |
| **Refinements** | Apply all refinements before extraction (resolve to final state) |
| **Extends** | Resolve inheritance before extraction (flatten to final state) |
| **Native derived tables** | Convert to SQL derived tables or materialize |
| **PDT persistence** | Note in metadata but don't replicate (persistence is adapter-specific) |
| **Drill fields** | Capture as metadata on measures but not core functionality |
| **HTML/Links** | Not extracted (presentation layer concern) |

---

## Principles for Converters

When building converters to/from other formats:

### From LookML → Story
1. Resolve all extends and refinements first (flatten to final state)
2. Evaluate Liquid templates where possible (flag dynamic ones)
3. Promote `type: number` measures to derived metrics
4. Extract `primary_key: yes` as `grain`
5. Parse `value_format_name` into `format` + `format_pattern`
6. Use LLM to enrich descriptions that are empty or terse
7. Profile the database to populate `known_values`, `cardinality`, `date_ranges`

### From dbt MetricFlow → Story
1. Semantic models map directly to tables
2. Entities provide explicit join information
3. Metrics map directly (same types, similar properties)
4. `non_additive_dimension` → `semi_additive`
5. Saved queries → join definitions or saved queries in our format

### Story → LookML (for backward compatibility during migration)
1. Tables → views with `sql_table_name`
2. Dimensions → dimensions (time → dimension_group)
3. Measures → measures with appropriate types
4. Derived metrics → `type: number` measures
5. Joins → joins with explicit join entries
6. Access control → access_grants + required_access_grants
7. Knowledge → descriptions (simplified — LookML can't hold rich context)

### Story → dbt MetricFlow
1. Tables → semantic_models
2. Entities extracted from explicit join definitions
3. Metrics map directly to dbt metric types
4. Cumulative/ratio/derived map naturally

---

## Query Interface

When querying the data context (via compiler or LLM), the interface is:

```
Query:
  metrics: [gmv, order_count, average_order_value]
  dimensions: [order_date.month, product_category]
  filters:
    order_status: delivered
    order_date: "2018-01-01 to 2018-06-30"
  sort: [order_date.month]
  limit: 100
```

The metric compiler resolves this to SQL:
1. Identify which tables are needed (from metric → measure → table)
2. Build the join graph (from join definitions or entity relationships)
3. Generate SELECT with correct aggregations
4. Apply filters
5. Validate against benchmarks if available

The LLM can also generate the query specification from natural language:
```
User: "Show me monthly revenue by category for the first half of 2018"
  → metrics: [gmv], dimensions: [order_date.month, product_category],
    filters: {order_date: "2018-01-01 to 2018-06-30"}
```

---

## Creation Workflow

How a data context gets built. The guiding principle: **AI generates, humans review.** No one writes YAML. No one stares at schema dumps. The system does the research and drafts the document; domain experts review it like they'd review a memo from an employee.

### Step 1: Connect & Discover (Fully Automatic)

User provides a database connection. The system crawls every table and profiles the data:
- Row counts, column types, cardinality, null rates
- Sample values, min/max ranges, date boundaries
- Foreign key detection (column name matching + value overlap analysis)
- Distribution analysis (skew, outliers, top values)

This is Tier 1 from our proof-of-concept work (31% metric name accuracy on its own). No user input needed — it runs in minutes.

### Step 2: Gather Context (Mostly Automatic)

The system automatically pulls as much context as it can from the database connection, then optionally accepts additional inputs. The minimum path is just the database connection — everything else makes the output better but is not required.

**Automatic (no user action needed):**
- **Query history** — Most warehouses (Snowflake `QUERY_HISTORY`, BigQuery `INFORMATION_SCHEMA.JOBS`, Redshift `STL_QUERYTEXT`) store recent SQL. The system pulls the last 90 days of queries directly from the warehouse, extracts business logic patterns, and identifies commonly-used metrics and joins. No one needs to Slack their colleagues or collect files manually.

**One-click (if available):**
- **GitHub repo scan** — User connects their GitHub org. The system scans for repos containing `.lkml` files or `dbt_project.yml` and presents a list: "We found these repos. Which ones are relevant?" One click, not a scavenger hunt.

**Optional (user provides if they have it):**
- Business documentation, data dictionaries, wiki pages, onboarding docs
- A plain-text description of the business ("We're a Brazilian e-commerce marketplace...")
- Additional SQL files or saved queries beyond what query history captured

Each input gets processed and compressed into structured context. More inputs → better output. The PoC showed business documentation is the single highest-impact input (Tier 3: 73% accuracy). But the minimum viable path — database profiling + query history — requires nothing from the user beyond a connection string.

### Step 3: Generate Draft (AI Synthesizes Everything)

The LLM synthesizes all inputs into a complete data context: tables, dimensions, measures, metrics, joins, knowledge base. The output is stored as YAML internally, but **users never see the YAML.**

### Step 4: Review (Readable Memo, Not YAML)

The system generates a **plain-language review document** — a bullet-point memo organized by business domain. This is what reviewers actually see:

```markdown
## Revenue Metrics

- **GMV** — Sum of product prices, excluding freight. Includes all order
  statuses. For realized revenue, use Realized GMV.
- **Realized GMV** — GMV filtered to delivered orders only (~97% of total).
- **AOV** — GMV divided by distinct order count. Currently ~R$137.
- **Freight Revenue** — Sum of freight charges across all line items.

## How Tables Connect

- Order Items → Customers (via customer_id, many-to-one)
- Order Items → Products (via product_id, many-to-one)
- Order Items → Sellers (via seller_id, many-to-one)

## Things to Watch Out For

- customer_id is NOT unique per person. Use customer_unique_id for counts.
- Price is per line item, not per order. An order with 3 items = 3 rows.
- Only ~10% of orders have reviews. Review data is biased toward extremes.
```

**Review process:**
- Works like a PR review or doc edit. Reviewers read the memo, make corrections in plain language ("We call this Net Revenue, not Realized GMV", "That join is wrong, we use order_unique_id now").
- Different domain experts review different sections — the revenue team reviews revenue metrics, the product team reviews product metrics. No single person needs to know everything.
- The AI takes the corrections and updates the data context. Reviewers never touch YAML.
- Multiple review rounds are fine. Each round produces a cleaner memo.

### Step 5: Validate (Mostly Automated)

Validation is primarily machine-driven. Humans should not be expected to confirm exact numbers — data changes constantly and no one knows the precise value of any metric at any given moment.

**Automated checks (no human needed):**
- Row count consistency across joined tables
- Foreign key integrity (do join keys actually match?)
- No-nonsense checks: negative revenue, future dates, NULL primary keys
- Internal consistency: do metrics that should be related actually add up? (e.g., GMV = Realized GMV + Canceled GMV)
- SQL compilation: does every metric definition produce valid, executable SQL?

**Cross-reference checks (minimal human input):**
- If existing reports or dashboards exist (Looker, Tableau, spreadsheets), compare key numbers. "Your Looker dashboard shows Q1 GMV was R$4.2M. Our calculation gives R$4.18M — within 0.5%, likely a rounding difference."
- Humans confirm order of magnitude ("yes, GMV is in the millions, not thousands") — not exact values.

**The output of validation is the `validated_examples` section** — but these are machine-generated benchmarks with tolerance ranges, not human-confirmed exact numbers.

### Incremental Updates

Adding a new table to the warehouse next month shouldn't require re-running the whole workflow:
- System detects schema changes (new tables, new columns, dropped columns)
- Generates a diff-style review memo: "3 new columns added to order_items. Recommended: add `shipping_method` as a categorical dimension."
- Same review process — domain expert approves or corrects the diff

---

*Specification version 0.1.0-draft*
*This is a living document. Version will increment as we validate against real-world LookML repos.*
